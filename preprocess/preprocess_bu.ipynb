{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transsexual-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from utils import *\n",
    "from dataset import RW_based_dataset, KG_based_dataset\n",
    "from knowledge_graph import RW_based_KG, KG_based_KG\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distant-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_class:\n",
    "    def __init__(self):\n",
    "        self.dataset = BEAUTY_CORE\n",
    "        self.att_th_lower = 0\n",
    "        self.att_th_upper = 3000\n",
    "        self.user_core_th = 6\n",
    "        self.user_top_k = 6000\n",
    "        \n",
    "args = args_class()\n",
    "        \n",
    "if not os.path.isdir(DATA_DIR[args.dataset]):\n",
    "    os.makedirs(DATA_DIR[args.dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-housing",
   "metadata": {},
   "source": [
    "load review to dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load', args.dataset, 'dataset from file...')\n",
    "dataset = RW_based_dataset(args, DATA_DIR[args.dataset] + '/review_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-inclusion",
   "metadata": {},
   "source": [
    "generate train and test label by filtered users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_filter(core_user_list, dataset, mode='train'):\n",
    "\n",
    "    review_file = '{}/{}/review_{}.txt.gz'.format(DATA_DIR[dataset], 'review_data', mode)\n",
    "    user_products = {}  # {uid: [pid,...], ...}\n",
    "\n",
    "    print('len(core_user_list) = ', len(core_user_list))\n",
    "\n",
    "    count = 0\n",
    "    with gzip.open(review_file, 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            line = line.decode('utf-8').strip()\n",
    "            arr = line.split('\\t')\n",
    "            user_idx = int(arr[0])\n",
    "            product_idx = int(arr[1])\n",
    "\n",
    "            if user_idx in core_user_list:\n",
    "                if user_idx not in user_products:\n",
    "                    user_products[user_idx] = []\n",
    "                user_products[user_idx].append(product_idx)\n",
    "                count += 1\n",
    "\n",
    "    print(mode + ', avg user product = ', count/len(user_products))\n",
    "\n",
    "    return user_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('generate filter label', args.dataset, 'knowledge graph from dataset...')\n",
    "core_user_list = dataset.core_user_list\n",
    "trn_label = labels_filter(core_user_list, args.dataset, 'train')\n",
    "tst_label = labels_filter(core_user_list, args.dataset, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-weekly",
   "metadata": {},
   "source": [
    "build KG from review dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('build', args.dataset, 'knowledge graph from dataset...')\n",
    "kg = RW_based_KG(args, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-tobago",
   "metadata": {},
   "source": [
    "save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.dataset, ' save dataset, trn tst label, kg')\n",
    "save_dataset(args.dataset, dataset)\n",
    "save_labels(args.dataset, trn_label, mode='train')\n",
    "save_labels(args.dataset, tst_label, mode='test')\n",
    "save_kg(args.dataset, kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-startup",
   "metadata": {},
   "source": [
    "part2 answer covering rate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-console",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "different-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding: ../data/Amazon_Beauty_Core/transe_embed.pkl\n",
      "label_file =  ../data/Amazon_Beauty_Core/train_label.pkl\n",
      "label_file =  ../data/Amazon_Beauty_Core/test_label.pkl\n"
     ]
    }
   ],
   "source": [
    "embeds = load_embed(args.dataset)\n",
    "kg = load_kg(args.dataset)\n",
    "trn_label = load_labels(args.dataset, 'train')\n",
    "tst_label = load_labels(args.dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "literary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def evaluate(topk_matches, test_user_products):\n",
    "    \"\"\"Compute metrics for predicted recommendations.\n",
    "    Args:\n",
    "        topk_matches: a list or dict of product ids in ascending order.\n",
    "    \"\"\"\n",
    "    cum_k = 0\n",
    "    invalid_users = []\n",
    "    # Compute metrics\n",
    "    precisions, recalls, ndcgs, hits = [], [], [], []\n",
    "    test_user_idxs = list(test_user_products.keys())\n",
    "    for uid in test_user_idxs:\n",
    "\n",
    "        if uid not in topk_matches:\n",
    "            print('uid not in topk_matches = ',uid)\n",
    "            invalid_users.append(uid)\n",
    "            continue\n",
    "        pred_list, rel_set = topk_matches[uid][::-1], test_user_products[uid]\n",
    "\n",
    "        if len(pred_list) == 0:\n",
    "            cum_k += 1\n",
    "            ndcgs.append(0)\n",
    "            recalls.append(0)\n",
    "            precisions.append(0)\n",
    "            hits.append(0)\n",
    "            continue\n",
    "\n",
    "        dcg = 0.0\n",
    "        hit_num = 0.0\n",
    "        for i in range(len(pred_list)):\n",
    "            if pred_list[i] in rel_set:\n",
    "                dcg += 1. / (log(i + 2) / log(2))\n",
    "                hit_num += 1\n",
    "        # idcg\n",
    "        idcg = 0.0\n",
    "        for i in range(min(len(rel_set), len(pred_list))):\n",
    "            idcg += 1. / (log(i + 2) / log(2))\n",
    "        ndcg = dcg / idcg\n",
    "\n",
    "        recall = hit_num / len(rel_set)\n",
    "\n",
    "        precision = hit_num / len(pred_list)\n",
    "\n",
    "        hit = 1.0 if hit_num > 0.0 else 0.0\n",
    "\n",
    "        ndcgs.append(ndcg)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        hits.append(hit)\n",
    "\n",
    "    avg_precision = np.mean(precisions) * 100\n",
    "    avg_recall = np.mean(recalls) * 100\n",
    "    avg_ndcg = np.mean(ndcgs) * 100\n",
    "    avg_hit = np.mean(hits) * 100\n",
    "    print('NDCG={:.3f} |  Recall={:.3f} | HR={:.3f} | Precision={:.3f} | Invalid users={}'.format(\n",
    "            avg_ndcg, avg_recall, avg_hit, avg_precision, len(invalid_users)))\n",
    "    print('cum_k == 0 ',  cum_k)\n",
    "    return avg_precision, avg_recall, avg_ndcg, avg_hit, invalid_users, cum_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "miniature-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acts = 51\n",
    "\n",
    "def get_actions(path, user):\n",
    "    \"\"\"Compute actions for current node.\"\"\"\n",
    "    curr_node_type, curr_node_id = path\n",
    "    actions = [(SELF_LOOP, curr_node_id)]  # self-loop must be included.\n",
    "\n",
    "    relations_nodes = kg(curr_node_type, curr_node_id)\n",
    "    candidate_acts = []  # list of tuples of (relation, node_type, node_id)\n",
    "\n",
    "    for r in relations_nodes:\n",
    "        next_node_type = KG_RELATION[curr_node_type][r]\n",
    "        next_node_ids = relations_nodes[r]\n",
    "        next_node_ids = [n for n in next_node_ids]  # filter\n",
    "        candidate_acts.extend(zip([r] * len(next_node_ids), next_node_ids))\n",
    "\n",
    "    # (3) If candidate action set is empty, only return self-loop action.\n",
    "    if len(candidate_acts) == 0:\n",
    "        actions = [(SELF_LOOP, curr_node_id)]\n",
    "        return actions\n",
    "\n",
    "    # (4) If number of available actions is smaller than max_acts, return action sets.\n",
    "    if len(candidate_acts) <= max_acts:\n",
    "        candidate_acts = sorted(candidate_acts, key=lambda x: (x[0], x[1]))\n",
    "        actions.extend(candidate_acts)\n",
    "        return actions\n",
    "\n",
    "    # (5) If there are too many actions, do some deterministic trimming here!\n",
    "    user_embed = embeds[USER][user]\n",
    "    scores = []\n",
    "    for r, next_node_id in candidate_acts:\n",
    "        next_node_type = KG_RELATION[curr_node_type][r]\n",
    "        if next_node_type == USER:\n",
    "            src_embed = user_embed\n",
    "        elif next_node_type == PRODUCT:\n",
    "            src_embed = user_embed + embeds[PURCHASE][0]\n",
    "        elif next_node_type == WORD:\n",
    "            src_embed = user_embed + embeds[MENTION][0]\n",
    "        else:\n",
    "            src_embed = user_embed + embeds[PURCHASE][0] + embeds[r][0]\n",
    "        score = np.matmul(src_embed, embeds[next_node_type][next_node_id])\n",
    "        scores.append(score)\n",
    "        \n",
    "    candidate_idxs = np.argsort(scores)[-max_acts:]  # choose actions with larger scores\n",
    "    candidate_acts = sorted([candidate_acts[i] for i in candidate_idxs], key=lambda x: (x[0], x[1]))\n",
    "    actions.extend(candidate_acts)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "electronic-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_cover_rate(step):\n",
    "    anser_cover = {}\n",
    "    for user, trn_item_list in trn_label.items():\n",
    "        total_item = [[USER, user]]\n",
    "        cache = {}\n",
    "        cache[USER] = {}\n",
    "        cache[USER][user] = 1\n",
    "        for _ in range(step):\n",
    "            total_item_tmp = []\n",
    "            for action in total_item:\n",
    "                next_action = get_actions(action, user)\n",
    "\n",
    "                for n_action in next_action:\n",
    "                    curr_node_type, _ = action\n",
    "                    relation, next_node_id = n_action\n",
    "                    if relation == SELF_LOOP: next_node_type = curr_node_type\n",
    "                    else: next_node_type = KG_RELATION[curr_node_type][relation]\n",
    "                    if next_node_type not in cache: cache[next_node_type] = {}\n",
    "                    if next_node_id not in cache[next_node_type]:\n",
    "                        cache[next_node_type][next_node_id] = 1\n",
    "                        total_item_tmp.append([next_node_type, next_node_id])\n",
    "            total_item = total_item_tmp\n",
    "            \n",
    "        if 'product' in cache:\n",
    "            anser_cover[user] = [it for it in cache['product'] if it not in trn_item_list]\n",
    "\n",
    "    evaluate(anser_cover, tst_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "international-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG=0.000 |  Recall=0.000 | HR=0.000 | Precision=0.000 | Invalid users=0\n",
      "cum_k == 0  8300\n"
     ]
    }
   ],
   "source": [
    "ans_cover_rate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "superior-polyester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG=6.008 |  Recall=16.626 | HR=39.120 | Precision=1.542 | Invalid users=0\n",
      "cum_k == 0  33\n"
     ]
    }
   ],
   "source": [
    "ans_cover_rate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rental-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG=10.703 |  Recall=67.463 | HR=92.012 | Precision=0.236 | Invalid users=0\n",
      "cum_k == 0  0\n"
     ]
    }
   ],
   "source": [
    "ans_cover_rate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-brighton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-ancient",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-police",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
