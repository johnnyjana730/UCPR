{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corrected-ceremony",
   "metadata": {},
   "source": [
    "# Running UCPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equal-burning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/johnnyjana730/github/UCPR/', '/home/johnnyjana730/github/UCPR/bash', '/home/johnnyjana730/anaconda3_tmp/lib/python38.zip', '/home/johnnyjana730/anaconda3_tmp/lib/python3.8', '/home/johnnyjana730/anaconda3_tmp/lib/python3.8/lib-dynload', '', '/home/johnnyjana730/anaconda3_tmp/lib/python3.8/site-packages', '/home/johnnyjana730/anaconda3_tmp/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg', '/home/johnnyjana730/anaconda3_tmp/lib/python3.8/site-packages/IPython/extensions', '/home/johnnyjana730/.ipython']\n",
      "dataset =  beauty_core\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'/home/johnnyjana730/github/UCPR/')\n",
    "print(sys.path)\n",
    "\n",
    "from config import get_hparams\n",
    "\n",
    "!export PYTHONPATH=\"./\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from config import get_hparams\n",
    "\n",
    "# cell_core, beauty_core, cloth_core, MovieLens-1M_core, amazon-book_20core\n",
    "\n",
    "DATASET = \"beauty_core\"\n",
    "\n",
    "locals().update(get_hparams(DATASET))\n",
    "\n",
    "epochs=100\n",
    "KGE_pretrained=1\n",
    "kg_emb_grad=0\n",
    "tri_wd_rm=1\n",
    "tri_pro_rm=0\n",
    "batch_size=32\n",
    "load_pretrain_model=0\n",
    "\n",
    "train_file=\"train.py\"\n",
    "test_file=\"test.py\"\n",
    "\n",
    "exp_name=f\"note_rm_w{tri_wd_rm}_p{tri_pro_rm}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-class",
   "metadata": {},
   "source": [
    "# Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "objective-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Command:\n",
      "python3 ../src/train.py --reasoning_step 2 --batch_size 32 --name note_rm_w1_p0 --lr 0.0001 --embed_size 32 --n_memory 64 --load_pretrain_model 0 --tri_wd_rm 1 --tri_pro_rm 0 --gp_setting 6_800_15_500_50 --epochs 100 --KGE_pretrained 1 --lambda_num 0.5 --kg_emb_grad 0 --p_hop 2 --reasoning_step 2 --model lstm --dataset beauty_core\n",
      "args.gp_setting =  6_800_15_500_50 args.att_core =  0 args.item_core =  0 args.user_core_th =  6 args.kg_fre_upper =  500 args.max_acts =  50\n",
      "len(self.core_user_list) =  8300\n",
      "self.args.core_user_list =  8300\n",
      "self.user_triplet_list =  8300\n",
      "Load embedding: ../data/Amazon_Beauty_Core/transe_embed.pkl\n",
      "env.output_valid_user() =  8300\n",
      "args.batch_size =  32\n",
      "Load embedding: ../data/Amazon_Beauty_Core/transe_embed.pkl\n",
      "self.embeds[key] =  (22363, 100)\n",
      "weight =  (22364, 32)\n",
      "key =  user\n",
      "self.embeds[key] =  torch.Size([22364, 32])\n",
      "vocab_size + 1 =  22364\n",
      "embed =  torch.Size([22364, 32])\n",
      "embed.requires_grad =  False\n",
      "self.embeds[key] =  (12101, 100)\n",
      "weight =  (12102, 32)\n",
      "key =  product\n",
      "self.embeds[key] =  torch.Size([12102, 32])\n",
      "vocab_size + 1 =  12102\n",
      "embed =  torch.Size([12102, 32])\n",
      "embed.requires_grad =  False\n",
      "self.embeds[key] =  (22564, 100)\n",
      "weight =  (22565, 32)\n",
      "key =  word\n",
      "self.embeds[key] =  torch.Size([22565, 32])\n",
      "vocab_size + 1 =  22565\n",
      "embed =  torch.Size([22565, 32])\n",
      "embed.requires_grad =  False\n",
      "self.embeds[key] =  (164721, 100)\n",
      "weight =  (164722, 32)\n",
      "key =  related_product\n",
      "self.embeds[key] =  torch.Size([164722, 32])\n",
      "vocab_size + 1 =  164722\n",
      "embed =  torch.Size([164722, 32])\n",
      "embed.requires_grad =  False\n",
      "self.embeds[key] =  (2077, 100)\n",
      "weight =  (2078, 32)\n",
      "key =  brand\n",
      "self.embeds[key] =  torch.Size([2078, 32])\n",
      "vocab_size + 1 =  2078\n",
      "embed =  torch.Size([2078, 32])\n",
      "embed.requires_grad =  False\n",
      "self.embeds[key] =  (248, 100)\n",
      "weight =  (249, 32)\n",
      "key =  category\n",
      "self.embeds[key] =  torch.Size([249, 32])\n",
      "vocab_size + 1 =  249\n",
      "embed =  torch.Size([249, 32])\n",
      "embed.requires_grad =  False\n",
      "r =  setup self_loop\n",
      "key =  self_loop\n",
      "self_loop embed =  torch.Size([1, 32])\n",
      "r =  setup purchase\n",
      "key =  purchase\n",
      "purchase embed =  torch.Size([1, 32])\n",
      "r =  setup mentions\n",
      "key =  mentions\n",
      "mentions embed =  torch.Size([1, 32])\n",
      "r =  setup described_as\n",
      "key =  described_as\n",
      "described_as embed =  torch.Size([1, 32])\n",
      "r =  setup produced_by\n",
      "key =  produced_by\n",
      "produced_by embed =  torch.Size([1, 32])\n",
      "r =  setup belongs_to\n",
      "key =  belongs_to\n",
      "belongs_to embed =  torch.Size([1, 32])\n",
      "r =  setup also_bought\n",
      "key =  also_bought\n",
      "also_bought embed =  torch.Size([1, 32])\n",
      "r =  setup also_viewed\n",
      "key =  also_viewed\n",
      "also_viewed embed =  torch.Size([1, 32])\n",
      "r =  setup bought_together\n",
      "key =  bought_together\n",
      "bought_together embed =  torch.Size([1, 32])\n",
      "r =  setup padding\n",
      "key =  padding\n",
      "padding embed =  torch.Size([1, 32])\n",
      "[INFO]  Namespace(KGE_pretrained=True, act_dropout=0.5, add_products=False, att_core=0, att_evaluation=False, batch_size=32, best_model_epoch=0, best_save_model_dir='', core_user_list='', dataset='beauty_core', device=device(type='cuda', index=0), embed_size=32, ent_weight=0.001, envir='p1', epochs=100, eva_epochs=0, gamma=0.99, gp_setting='6_800_15_500_50', gpu='0', gradient_plot='gradient_plot/', h0_embbed=0, hidden=[64, 32], item_core=0, kg_emb_grad=False, kg_fre_dict='', kg_fre_lower=15, kg_fre_upper=500, kg_no_grad=False, l2_lambda=0, l2_weight=1e-06, lambda_num=0.5, load_pretrain_model=False, load_pt_emb_size=False, log_dir='../eva/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300', logger=<Logger ../eva/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/train_log.txt (DEBUG)>, lr=0.0001, max_acts=50, max_path_len=3, model='lstm', n_memory=64, name='note_rm_w1_p0', non_sampling=True, p_hop=2, pretest=False, pretrained_dir='../eva/Amazon_Beauty_Core/pretrained/note_rm_w1_p0_g_aiu_0_0_300', pretrained_st_epoch=0, reasoning_step=2, reward_hybrid=False, reward_rh='', run_eval=True, run_path=True, sam_type='alet', save_model_dir='../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300', seed=52, sort_by='score', state_history=1, state_rg=False, sub_batch_size=1, test_lstm_up=True, topk=[10, 10, 1], topk_list=[1, 10, 100, 100], topk_string='10, 10, 1', training=True, tri_pro_rm=False, tri_wd_rm=True, user_core=300, user_core_th=6)\n",
      "[INFO]  Parameters:['kg_emb.self_loop', 'kg_emb.purchase', 'kg_emb.mentions', 'kg_emb.described_as', 'kg_emb.produced_by', 'kg_emb.belongs_to', 'kg_emb.also_bought', 'kg_emb.also_viewed', 'kg_emb.bought_together', 'kg_emb.padding', 'kg_emb.user.weight', 'kg_emb.product.weight', 'kg_emb.word.weight', 'kg_emb.related_product.weight', 'kg_emb.brand.weight', 'kg_emb.category.weight', 'kg_emb.self_loop_bias.weight', 'kg_emb.purchase_bias.weight', 'kg_emb.mentions_bias.weight', 'kg_emb.described_as_bias.weight', 'kg_emb.produced_by_bias.weight', 'kg_emb.belongs_to_bias.weight', 'kg_emb.also_bought_bias.weight', 'kg_emb.also_viewed_bias.weight', 'kg_emb.bought_together_bias.weight', 'kg_emb.padding_bias.weight', 'state_lstm.policy_lstm.lstm.weight_ih_l0', 'state_lstm.policy_lstm.lstm.weight_hh_l0', 'state_lstm.policy_lstm.lstm.bias_ih_l0', 'state_lstm.policy_lstm.lstm.bias_hh_l0', 'transfor_state.weight', 'transfor_state.bias', 'state_tr_query.weight', 'state_tr_query.bias', 'l1.weight', 'l1.bias', 'l2.weight', 'l2.bias', 'actor.weight', 'actor.bias', 'critic.weight', 'critic.bias']\n",
      "[INFO]  epoch/step=0/100 | loss=0.14076 | ploss=0.11384 | vloss=0.03492 | entropy=-9.56853 | reward=0.01188\n",
      "[INFO]  epoch/step=0/200 | loss=0.12747 | ploss=0.10331 | vloss=0.03216 | entropy=-9.56799 | reward=0.01094\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_0.ckpt\n",
      "[INFO]  epoch/step=1/300 | loss=0.13698 | ploss=0.11006 | vloss=0.03492 | entropy=-9.55546 | reward=0.01188\n",
      "[INFO]  epoch/step=1/400 | loss=0.21011 | ploss=0.16382 | vloss=0.05422 | entropy=-9.48985 | reward=0.01844\n",
      "[INFO]  epoch/step=1/500 | loss=0.23345 | ploss=0.17975 | vloss=0.06157 | entropy=-9.42104 | reward=0.02094\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_1.ckpt\n",
      "[INFO]  epoch/step=2/600 | loss=0.29853 | ploss=0.22630 | vloss=0.07995 | entropy=-9.26341 | reward=0.02719\n",
      "[INFO]  epoch/step=2/700 | loss=0.31401 | ploss=0.23413 | vloss=0.08730 | entropy=-8.95815 | reward=0.02969\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_2.ckpt\n",
      "[INFO]  epoch/step=3/800 | loss=0.42216 | ploss=0.30967 | vloss=0.11947 | entropy=-8.51556 | reward=0.04063\n",
      "[INFO]  epoch/step=3/900 | loss=0.41702 | ploss=0.29577 | vloss=0.12774 | entropy=-8.00959 | reward=0.04344\n",
      "[INFO]  epoch/step=3/1000 | loss=0.56671 | ploss=0.39341 | vloss=0.17920 | entropy=-7.41583 | reward=0.06094\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_3.ckpt\n",
      "[INFO]  epoch/step=4/1100 | loss=0.63842 | ploss=0.43610 | vloss=0.20769 | entropy=-6.89578 | reward=0.07062\n",
      "[INFO]  epoch/step=4/1200 | loss=0.73654 | ploss=0.49255 | vloss=0.24904 | entropy=-6.56512 | reward=0.08469\n",
      "[INFO]  epoch/step=4/1300 | loss=0.75615 | ploss=0.49607 | vloss=0.26497 | entropy=-6.40294 | reward=0.08906\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_4.ckpt\n",
      "[INFO]  epoch/step=5/1400 | loss=0.84428 | ploss=0.55135 | vloss=0.29775 | entropy=-6.32362 | reward=0.10125\n",
      "[INFO]  epoch/step=5/1500 | loss=0.82758 | ploss=0.54291 | vloss=0.28947 | entropy=-6.31620 | reward=0.09844\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_5.ckpt\n",
      "[INFO]  epoch/step=6/1600 | loss=1.00035 | ploss=0.64727 | vloss=0.35778 | entropy=-6.21309 | reward=0.12062\n",
      "[INFO]  epoch/step=6/1700 | loss=0.92740 | ploss=0.59202 | vloss=0.34002 | entropy=-6.13352 | reward=0.11563\n",
      "[INFO]  epoch/step=6/1800 | loss=0.93502 | ploss=0.59591 | vloss=0.34369 | entropy=-6.09215 | reward=0.11688\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_6.ckpt\n",
      "[INFO]  epoch/step=7/1900 | loss=1.04842 | ploss=0.66391 | vloss=0.38903 | entropy=-6.01487 | reward=0.13125\n",
      "[INFO]  epoch/step=7/2000 | loss=0.98826 | ploss=0.62333 | vloss=0.36942 | entropy=-5.99005 | reward=0.12562\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_7.ckpt\n",
      "[INFO]  epoch/step=8/2100 | loss=1.02717 | ploss=0.64262 | vloss=0.38903 | entropy=-5.97097 | reward=0.13125\n",
      "[INFO]  epoch/step=8/2200 | loss=1.10126 | ploss=0.69124 | vloss=0.41445 | entropy=-5.92721 | reward=0.14094\n",
      "[INFO]  epoch/step=8/2300 | loss=1.10861 | ploss=0.69397 | vloss=0.41905 | entropy=-5.89892 | reward=0.14250\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_8.ckpt\n",
      "[INFO]  epoch/step=9/2400 | loss=1.06064 | ploss=0.65209 | vloss=0.41292 | entropy=-5.87172 | reward=0.13937\n",
      "[INFO]  epoch/step=9/2500 | loss=1.10376 | ploss=0.67801 | vloss=0.43008 | entropy=-5.81810 | reward=0.14625\n",
      "[INFO]  epoch/step=9/2600 | loss=1.19360 | ploss=0.72612 | vloss=0.47174 | entropy=-5.73851 | reward=0.15781\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_9.ckpt\n",
      "[INFO]  epoch/step=10/2700 | loss=1.15969 | ploss=0.70538 | vloss=0.45856 | entropy=-5.73480 | reward=0.15594\n",
      "[INFO]  epoch/step=10/2800 | loss=1.08607 | ploss=0.65287 | vloss=0.43743 | entropy=-5.71082 | reward=0.14875\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_10.ckpt\n",
      "[INFO]  epoch/step=11/2900 | loss=1.17630 | ploss=0.71240 | vloss=0.46806 | entropy=-5.64044 | reward=0.15812\n",
      "[INFO]  epoch/step=11/3000 | loss=1.18445 | ploss=0.70886 | vloss=0.47970 | entropy=-5.59336 | reward=0.16312\n",
      "[INFO]  epoch/step=11/3100 | loss=1.30769 | ploss=0.78430 | vloss=0.52749 | entropy=-5.57173 | reward=0.17938\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_11.ckpt\n",
      "[INFO]  epoch/step=12/3200 | loss=1.28953 | ploss=0.77468 | vloss=0.51891 | entropy=-5.53839 | reward=0.17594\n",
      "[INFO]  epoch/step=12/3300 | loss=1.29012 | ploss=0.77220 | vloss=0.52197 | entropy=-5.52922 | reward=0.17750\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_12.ckpt\n",
      "[INFO]  epoch/step=13/3400 | loss=1.34333 | ploss=0.80426 | vloss=0.54311 | entropy=-5.50928 | reward=0.18313\n",
      "[INFO]  epoch/step=13/3500 | loss=1.34954 | ploss=0.80768 | vloss=0.54587 | entropy=-5.48502 | reward=0.18563\n",
      "[INFO]  epoch/step=13/3600 | loss=1.27179 | ploss=0.75936 | vloss=0.51646 | entropy=-5.50207 | reward=0.17563\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_13.ckpt\n",
      "[INFO]  epoch/step=14/3700 | loss=1.38434 | ploss=0.82502 | vloss=0.56333 | entropy=-5.48148 | reward=0.19000\n",
      "[INFO]  epoch/step=14/3800 | loss=1.38672 | ploss=0.81819 | vloss=0.57252 | entropy=-5.45490 | reward=0.19469\n",
      "[INFO]  epoch/step=14/3900 | loss=1.39778 | ploss=0.83296 | vloss=0.56884 | entropy=-5.48390 | reward=0.19187\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_14.ckpt\n",
      "[INFO]  epoch/step=15/4000 | loss=1.35338 | ploss=0.79859 | vloss=0.55873 | entropy=-5.40669 | reward=0.19000\n",
      "[INFO]  epoch/step=15/4100 | loss=1.37678 | ploss=0.81466 | vloss=0.56608 | entropy=-5.43412 | reward=0.19250\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_15.ckpt\n",
      "[INFO]  epoch/step=16/4200 | loss=1.36871 | ploss=0.81242 | vloss=0.56026 | entropy=-5.43782 | reward=0.19000\n",
      "[INFO]  epoch/step=16/4300 | loss=1.30015 | ploss=0.76097 | vloss=0.54311 | entropy=-5.39194 | reward=0.18469\n",
      "[INFO]  epoch/step=16/4400 | loss=1.30432 | ploss=0.75228 | vloss=0.55598 | entropy=-5.39161 | reward=0.18906\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_16.ckpt\n",
      "[INFO]  epoch/step=17/4500 | loss=1.31742 | ploss=0.77273 | vloss=0.54862 | entropy=-5.39592 | reward=0.18656\n",
      "[INFO]  epoch/step=17/4600 | loss=1.43106 | ploss=0.84040 | vloss=0.59457 | entropy=-5.37070 | reward=0.20219\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_17.ckpt\n",
      "[INFO]  epoch/step=18/4700 | loss=1.31127 | ploss=0.77302 | vloss=0.54219 | entropy=-5.39801 | reward=0.18281\n",
      "[INFO]  epoch/step=18/4800 | loss=1.36627 | ploss=0.81153 | vloss=0.55873 | entropy=-5.44261 | reward=0.19000\n",
      "[INFO]  epoch/step=18/4900 | loss=1.28854 | ploss=0.75209 | vloss=0.54035 | entropy=-5.36416 | reward=0.18375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_18.ckpt\n",
      "[INFO]  epoch/step=19/5000 | loss=1.40522 | ploss=0.82034 | vloss=0.58875 | entropy=-5.32659 | reward=0.19812\n",
      "[INFO]  epoch/step=19/5100 | loss=1.37045 | ploss=0.80185 | vloss=0.57252 | entropy=-5.36356 | reward=0.19469\n",
      "[INFO]  epoch/step=19/5200 | loss=1.41436 | ploss=0.82580 | vloss=0.59243 | entropy=-5.31764 | reward=0.20094\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_19.ckpt\n",
      "[INFO]  epoch/step=20/5300 | loss=1.42620 | ploss=0.82450 | vloss=0.60560 | entropy=-5.34460 | reward=0.20594\n",
      "[INFO]  epoch/step=20/5400 | loss=1.41812 | ploss=0.83017 | vloss=0.59182 | entropy=-5.31495 | reward=0.20125\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_20.ckpt\n",
      "[INFO]  epoch/step=21/5500 | loss=1.33978 | ploss=0.77325 | vloss=0.57037 | entropy=-5.29215 | reward=0.19344\n",
      "[INFO]  epoch/step=21/5600 | loss=1.43972 | ploss=0.83611 | vloss=0.60744 | entropy=-5.27590 | reward=0.20656\n",
      "[INFO]  epoch/step=21/5700 | loss=1.41302 | ploss=0.83153 | vloss=0.58538 | entropy=-5.33017 | reward=0.19906\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_21.ckpt\n",
      "[INFO]  epoch/step=22/5800 | loss=1.46579 | ploss=0.85300 | vloss=0.61663 | entropy=-5.28070 | reward=0.20813\n",
      "[INFO]  epoch/step=22/5900 | loss=1.36369 | ploss=0.79135 | vloss=0.57619 | entropy=-5.28863 | reward=0.19594\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_22.ckpt\n",
      "[INFO]  epoch/step=23/6000 | loss=1.38185 | ploss=0.79631 | vloss=0.58936 | entropy=-5.25568 | reward=0.19937\n",
      "[INFO]  epoch/step=23/6100 | loss=1.46595 | ploss=0.84765 | vloss=0.62214 | entropy=-5.27504 | reward=0.21156\n",
      "[INFO]  epoch/step=23/6200 | loss=1.38490 | ploss=0.81072 | vloss=0.57803 | entropy=-5.28652 | reward=0.19656\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_23.ckpt\n",
      "[INFO]  epoch/step=24/6300 | loss=1.37567 | ploss=0.79502 | vloss=0.58446 | entropy=-5.25037 | reward=0.19875\n",
      "[INFO]  epoch/step=24/6400 | loss=1.41040 | ploss=0.80952 | vloss=0.60468 | entropy=-5.24368 | reward=0.20563\n",
      "[INFO]  epoch/step=24/6500 | loss=1.42406 | ploss=0.82009 | vloss=0.60774 | entropy=-5.21204 | reward=0.20563\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_24.ckpt\n",
      "[INFO]  epoch/step=25/6600 | loss=1.41331 | ploss=0.80502 | vloss=0.61203 | entropy=-5.17520 | reward=0.20813\n",
      "[INFO]  epoch/step=25/6700 | loss=1.40912 | ploss=0.80822 | vloss=0.60468 | entropy=-5.20674 | reward=0.20563\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_25.ckpt\n",
      "[INFO]  epoch/step=26/6800 | loss=1.42842 | ploss=0.81982 | vloss=0.61234 | entropy=-5.16607 | reward=0.20719\n",
      "[INFO]  epoch/step=26/6900 | loss=1.42870 | ploss=0.81947 | vloss=0.61295 | entropy=-5.14485 | reward=0.20844\n",
      "[INFO]  epoch/step=26/7000 | loss=1.36838 | ploss=0.78490 | vloss=0.58722 | entropy=-5.16812 | reward=0.19969\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_26.ckpt\n",
      "[INFO]  epoch/step=27/7100 | loss=1.42425 | ploss=0.82211 | vloss=0.60591 | entropy=-5.19674 | reward=0.20500\n",
      "[INFO]  epoch/step=27/7200 | loss=1.47823 | ploss=0.84047 | vloss=0.64144 | entropy=-5.10054 | reward=0.21813\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_27.ckpt\n",
      "[INFO]  epoch/step=28/7300 | loss=1.40661 | ploss=0.80563 | vloss=0.60468 | entropy=-5.12159 | reward=0.20406\n",
      "[INFO]  epoch/step=28/7400 | loss=1.43959 | ploss=0.82206 | vloss=0.62122 | entropy=-5.12237 | reward=0.21125\n",
      "[INFO]  epoch/step=28/7500 | loss=1.42192 | ploss=0.79701 | vloss=0.62857 | entropy=-5.08624 | reward=0.21375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_28.ckpt\n",
      "[INFO]  epoch/step=29/7600 | loss=1.47586 | ploss=0.83747 | vloss=0.64205 | entropy=-5.08900 | reward=0.21781\n",
      "[INFO]  epoch/step=29/7700 | loss=1.38048 | ploss=0.77852 | vloss=0.60560 | entropy=-5.06339 | reward=0.20594\n",
      "[INFO]  epoch/step=29/7800 | loss=1.45078 | ploss=0.82433 | vloss=0.63011 | entropy=-5.07574 | reward=0.21375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_29.ckpt\n",
      "[INFO]  epoch/step=30/7900 | loss=1.44328 | ploss=0.81374 | vloss=0.63317 | entropy=-5.03956 | reward=0.21531\n",
      "[INFO]  epoch/step=30/8000 | loss=1.45259 | ploss=0.82303 | vloss=0.63317 | entropy=-5.02442 | reward=0.21531\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_30.ckpt\n",
      "[INFO]  epoch/step=31/8100 | loss=1.36550 | ploss=0.76172 | vloss=0.60744 | entropy=-5.06917 | reward=0.20500\n",
      "[INFO]  epoch/step=31/8200 | loss=1.46566 | ploss=0.82413 | vloss=0.64512 | entropy=-5.00121 | reward=0.21937\n",
      "[INFO]  epoch/step=31/8300 | loss=1.37665 | ploss=0.77742 | vloss=0.60284 | entropy=-5.02804 | reward=0.20500\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_31.ckpt\n",
      "[INFO]  epoch/step=32/8400 | loss=1.41209 | ploss=0.79171 | vloss=0.62398 | entropy=-5.01301 | reward=0.21063\n",
      "[INFO]  epoch/step=32/8500 | loss=1.46978 | ploss=0.82734 | vloss=0.64603 | entropy=-5.00913 | reward=0.21969\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_32.ckpt\n",
      "[INFO]  epoch/step=33/8600 | loss=1.45337 | ploss=0.81335 | vloss=0.64358 | entropy=-4.97337 | reward=0.21625\n",
      "[INFO]  epoch/step=33/8700 | loss=1.44791 | ploss=0.80636 | vloss=0.64512 | entropy=-4.97186 | reward=0.21937\n",
      "[INFO]  epoch/step=33/8800 | loss=1.39521 | ploss=0.78123 | vloss=0.61755 | entropy=-4.98146 | reward=0.21000\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_33.ckpt\n",
      "[INFO]  epoch/step=34/8900 | loss=1.46348 | ploss=0.81704 | vloss=0.65002 | entropy=-4.98017 | reward=0.21844\n",
      "[INFO]  epoch/step=34/9000 | loss=1.50591 | ploss=0.85517 | vloss=0.65430 | entropy=-4.97602 | reward=0.22250\n",
      "[INFO]  epoch/step=34/9100 | loss=1.47201 | ploss=0.82369 | vloss=0.65185 | entropy=-4.94213 | reward=0.22062\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_34.ckpt\n",
      "[INFO]  epoch/step=35/9200 | loss=1.51918 | ploss=0.84080 | vloss=0.68187 | entropy=-4.89759 | reward=0.23187\n",
      "[INFO]  epoch/step=35/9300 | loss=1.39139 | ploss=0.77368 | vloss=0.62122 | entropy=-4.91703 | reward=0.21125\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_35.ckpt\n",
      "[INFO]  epoch/step=36/9400 | loss=1.44996 | ploss=0.79179 | vloss=0.66166 | entropy=-4.89240 | reward=0.22344\n",
      "[INFO]  epoch/step=36/9500 | loss=1.47089 | ploss=0.81092 | vloss=0.66349 | entropy=-4.91757 | reward=0.22562\n",
      "[INFO]  epoch/step=36/9600 | loss=1.42555 | ploss=0.80050 | vloss=0.62857 | entropy=-4.91744 | reward=0.21375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_36.ckpt\n",
      "[INFO]  epoch/step=37/9700 | loss=1.49862 | ploss=0.83711 | vloss=0.66503 | entropy=-4.90818 | reward=0.22562\n",
      "[INFO]  epoch/step=37/9800 | loss=1.42051 | ploss=0.78988 | vloss=0.63409 | entropy=-4.85781 | reward=0.21563\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_37.ckpt\n",
      "[INFO]  epoch/step=38/9900 | loss=1.52277 | ploss=0.84714 | vloss=0.67912 | entropy=-4.87901 | reward=0.22937\n",
      "[INFO]  epoch/step=38/10000 | loss=1.39231 | ploss=0.75986 | vloss=0.63593 | entropy=-4.87175 | reward=0.21625\n",
      "[INFO]  epoch/step=38/10100 | loss=1.49587 | ploss=0.82574 | vloss=0.67360 | entropy=-4.85963 | reward=0.22906\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_38.ckpt\n",
      "[INFO]  epoch/step=39/10200 | loss=1.53366 | ploss=0.85219 | vloss=0.68494 | entropy=-4.85985 | reward=0.23031\n",
      "[INFO]  epoch/step=39/10300 | loss=1.43767 | ploss=0.78590 | vloss=0.65522 | entropy=-4.85089 | reward=0.22281\n",
      "[INFO]  epoch/step=39/10400 | loss=1.38359 | ploss=0.75723 | vloss=0.62980 | entropy=-4.82392 | reward=0.21313\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_39.ckpt\n",
      "[INFO]  epoch/step=40/10500 | loss=1.39901 | ploss=0.77298 | vloss=0.62949 | entropy=-4.85008 | reward=0.21406\n",
      "[INFO]  epoch/step=40/10600 | loss=1.43701 | ploss=0.78058 | vloss=0.65982 | entropy=-4.78019 | reward=0.22437\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_40.ckpt\n",
      "[INFO]  epoch/step=41/10700 | loss=1.45798 | ploss=0.80247 | vloss=0.65890 | entropy=-4.77112 | reward=0.22250\n",
      "[INFO]  epoch/step=41/10800 | loss=1.52775 | ploss=0.84007 | vloss=0.69106 | entropy=-4.76690 | reward=0.23500\n",
      "[INFO]  epoch/step=41/10900 | loss=1.41727 | ploss=0.77370 | vloss=0.64695 | entropy=-4.77130 | reward=0.22000\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_41.ckpt\n",
      "[INFO]  epoch/step=42/11000 | loss=1.42473 | ploss=0.76766 | vloss=0.66043 | entropy=-4.74668 | reward=0.22406\n",
      "[INFO]  epoch/step=42/11100 | loss=1.41492 | ploss=0.76030 | vloss=0.65798 | entropy=-4.74067 | reward=0.22375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_42.ckpt\n",
      "[INFO]  epoch/step=43/11200 | loss=1.49760 | ploss=0.81355 | vloss=0.68739 | entropy=-4.71759 | reward=0.23219\n",
      "[INFO]  epoch/step=43/11300 | loss=1.49174 | ploss=0.81227 | vloss=0.68279 | entropy=-4.70646 | reward=0.23219\n",
      "[INFO]  epoch/step=43/11400 | loss=1.47056 | ploss=0.79385 | vloss=0.68004 | entropy=-4.70556 | reward=0.23125\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_43.ckpt\n",
      "[INFO]  epoch/step=44/11500 | loss=1.49056 | ploss=0.80648 | vloss=0.68739 | entropy=-4.68487 | reward=0.23219\n",
      "[INFO]  epoch/step=44/11600 | loss=1.47630 | ploss=0.79867 | vloss=0.68095 | entropy=-4.70042 | reward=0.23156\n",
      "[INFO]  epoch/step=44/11700 | loss=1.48640 | ploss=0.81182 | vloss=0.67789 | entropy=-4.68936 | reward=0.22844\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_44.ckpt\n",
      "[INFO]  epoch/step=45/11800 | loss=1.49011 | ploss=0.80971 | vloss=0.68371 | entropy=-4.68764 | reward=0.23250\n",
      "[INFO]  epoch/step=45/11900 | loss=1.42179 | ploss=0.75792 | vloss=0.66717 | entropy=-4.67465 | reward=0.22687\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_45.ckpt\n",
      "[INFO]  epoch/step=46/12000 | loss=1.46109 | ploss=0.79045 | vloss=0.67391 | entropy=-4.64638 | reward=0.22656\n",
      "[INFO]  epoch/step=46/12100 | loss=1.43197 | ploss=0.77363 | vloss=0.66166 | entropy=-4.69464 | reward=0.22500\n",
      "[INFO]  epoch/step=46/12200 | loss=1.52092 | ploss=0.81749 | vloss=0.70669 | entropy=-4.62880 | reward=0.24031\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_46.ckpt\n",
      "[INFO]  epoch/step=47/12300 | loss=1.40372 | ploss=0.74502 | vloss=0.66196 | entropy=-4.63059 | reward=0.22250\n",
      "[INFO]  epoch/step=47/12400 | loss=1.49987 | ploss=0.80102 | vloss=0.70209 | entropy=-4.61273 | reward=0.23875\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_47.ckpt\n",
      "[INFO]  epoch/step=48/12500 | loss=1.52066 | ploss=0.82460 | vloss=0.69933 | entropy=-4.63995 | reward=0.23781\n",
      "[INFO]  epoch/step=48/12600 | loss=1.54130 | ploss=0.83421 | vloss=0.71036 | entropy=-4.64057 | reward=0.24156\n",
      "[INFO]  epoch/step=48/12700 | loss=1.50546 | ploss=0.80290 | vloss=0.70577 | entropy=-4.57728 | reward=0.24000\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_48.ckpt\n",
      "[INFO]  epoch/step=49/12800 | loss=1.50427 | ploss=0.80940 | vloss=0.69811 | entropy=-4.60976 | reward=0.23531\n",
      "[INFO]  epoch/step=49/12900 | loss=1.54105 | ploss=0.82654 | vloss=0.71771 | entropy=-4.56674 | reward=0.24406\n",
      "[INFO]  epoch/step=49/13000 | loss=1.50851 | ploss=0.80197 | vloss=0.70975 | entropy=-4.57154 | reward=0.24031\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_49.ckpt\n",
      "[INFO]  epoch/step=50/13100 | loss=1.49488 | ploss=0.80064 | vloss=0.69750 | entropy=-4.62034 | reward=0.23719\n",
      "[INFO]  epoch/step=50/13200 | loss=1.54046 | ploss=0.82592 | vloss=0.71771 | entropy=-4.54327 | reward=0.24406\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_50.ckpt\n",
      "[INFO]  epoch/step=51/13300 | loss=1.60241 | ploss=0.85261 | vloss=0.75294 | entropy=-4.50491 | reward=0.25344\n",
      "[INFO]  epoch/step=51/13400 | loss=1.49136 | ploss=0.78786 | vloss=0.70669 | entropy=-4.53947 | reward=0.24031\n",
      "[INFO]  epoch/step=51/13500 | loss=1.43476 | ploss=0.76344 | vloss=0.67452 | entropy=-4.56055 | reward=0.22937\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_51.ckpt\n",
      "[INFO]  epoch/step=52/13600 | loss=1.47395 | ploss=0.78087 | vloss=0.69627 | entropy=-4.55198 | reward=0.23469\n",
      "[INFO]  epoch/step=52/13700 | loss=1.49593 | ploss=0.79333 | vloss=0.70577 | entropy=-4.53504 | reward=0.24000\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_52.ckpt\n",
      "[INFO]  epoch/step=53/13800 | loss=1.53619 | ploss=0.81887 | vloss=0.72047 | entropy=-4.51231 | reward=0.24344\n",
      "[INFO]  epoch/step=53/13900 | loss=1.49857 | ploss=0.79044 | vloss=0.71128 | entropy=-4.51261 | reward=0.24188\n",
      "[INFO]  epoch/step=53/14000 | loss=1.43788 | ploss=0.75365 | vloss=0.68739 | entropy=-4.51936 | reward=0.23375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_53.ckpt\n",
      "[INFO]  epoch/step=54/14100 | loss=1.52313 | ploss=0.79936 | vloss=0.72690 | entropy=-4.48078 | reward=0.24563\n",
      "[INFO]  epoch/step=54/14200 | loss=1.50226 | ploss=0.78028 | vloss=0.72507 | entropy=-4.43873 | reward=0.24656\n",
      "[INFO]  epoch/step=54/14300 | loss=1.61145 | ploss=0.84934 | vloss=0.76519 | entropy=-4.43578 | reward=0.25812\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_54.ckpt\n",
      "[INFO]  epoch/step=55/14400 | loss=1.40741 | ploss=0.72497 | vloss=0.68555 | entropy=-4.46541 | reward=0.23312\n",
      "[INFO]  epoch/step=55/14500 | loss=1.47238 | ploss=0.76325 | vloss=0.71220 | entropy=-4.42067 | reward=0.24219\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_55.ckpt\n",
      "[INFO]  epoch/step=56/14600 | loss=1.41957 | ploss=0.74014 | vloss=0.68249 | entropy=-4.41575 | reward=0.23156\n",
      "[INFO]  epoch/step=56/14700 | loss=1.51445 | ploss=0.79247 | vloss=0.72507 | entropy=-4.43594 | reward=0.24656\n",
      "[INFO]  epoch/step=56/14800 | loss=1.43756 | ploss=0.74676 | vloss=0.69382 | entropy=-4.37514 | reward=0.23594\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_56.ckpt\n",
      "[INFO]  epoch/step=57/14900 | loss=1.40532 | ploss=0.72284 | vloss=0.68555 | entropy=-4.42268 | reward=0.23156\n",
      "[INFO]  epoch/step=57/15000 | loss=1.37672 | ploss=0.70800 | vloss=0.67177 | entropy=-4.39126 | reward=0.22844\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_57.ckpt\n",
      "[INFO]  epoch/step=58/15100 | loss=1.51231 | ploss=0.79031 | vloss=0.72507 | entropy=-4.41955 | reward=0.24344\n",
      "[INFO]  epoch/step=58/15200 | loss=1.51195 | ploss=0.76969 | vloss=0.74528 | entropy=-4.36468 | reward=0.25344\n",
      "[INFO]  epoch/step=58/15300 | loss=1.50686 | ploss=0.76365 | vloss=0.74620 | entropy=-4.34249 | reward=0.25375\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_58.ckpt\n",
      "[INFO]  epoch/step=59/15400 | loss=1.52514 | ploss=0.78775 | vloss=0.74038 | entropy=-4.33745 | reward=0.25125\n",
      "[INFO]  epoch/step=59/15500 | loss=1.44305 | ploss=0.74304 | vloss=0.70301 | entropy=-4.34142 | reward=0.23906\n",
      "[INFO]  epoch/step=59/15600 | loss=1.48233 | ploss=0.75844 | vloss=0.72690 | entropy=-4.36001 | reward=0.24563\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_59.ckpt\n",
      "[INFO]  epoch/step=60/15700 | loss=1.48583 | ploss=0.76925 | vloss=0.71955 | entropy=-4.31759 | reward=0.24469\n",
      "[INFO]  epoch/step=60/15800 | loss=1.52833 | ploss=0.78694 | vloss=0.74436 | entropy=-4.32044 | reward=0.25312\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_60.ckpt\n",
      "[INFO]  epoch/step=61/15900 | loss=1.47097 | ploss=0.75774 | vloss=0.71618 | entropy=-4.28874 | reward=0.24250\n",
      "[INFO]  epoch/step=61/16000 | loss=1.47459 | ploss=0.75613 | vloss=0.72139 | entropy=-4.26509 | reward=0.24531\n",
      "[INFO]  epoch/step=61/16100 | loss=1.50556 | ploss=0.75858 | vloss=0.74988 | entropy=-4.23633 | reward=0.25500\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_61.ckpt\n",
      "[INFO]  epoch/step=62/16200 | loss=1.45287 | ploss=0.73995 | vloss=0.71588 | entropy=-4.29266 | reward=0.24188\n",
      "[INFO]  epoch/step=62/16300 | loss=1.48309 | ploss=0.76740 | vloss=0.71863 | entropy=-4.27835 | reward=0.24438\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_62.ckpt\n",
      "[INFO]  epoch/step=63/16400 | loss=1.49197 | ploss=0.76184 | vloss=0.73303 | entropy=-4.23997 | reward=0.24719\n",
      "[INFO]  epoch/step=63/16500 | loss=1.40413 | ploss=0.70775 | vloss=0.69933 | entropy=-4.28998 | reward=0.23781\n",
      "[INFO]  epoch/step=63/16600 | loss=1.47555 | ploss=0.74141 | vloss=0.73701 | entropy=-4.21057 | reward=0.25062\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_63.ckpt\n",
      "[INFO]  epoch/step=64/16700 | loss=1.47290 | ploss=0.74829 | vloss=0.72752 | entropy=-4.23837 | reward=0.24688\n",
      "[INFO]  epoch/step=64/16800 | loss=1.53762 | ploss=0.78417 | vloss=0.75631 | entropy=-4.19684 | reward=0.25719\n",
      "[INFO]  epoch/step=64/16900 | loss=1.51426 | ploss=0.76417 | vloss=0.75294 | entropy=-4.18524 | reward=0.25344\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_64.ckpt\n",
      "[INFO]  epoch/step=65/17000 | loss=1.53271 | ploss=0.77929 | vloss=0.75631 | entropy=-4.22244 | reward=0.25719\n",
      "[INFO]  epoch/step=65/17100 | loss=1.53315 | ploss=0.79167 | vloss=0.74436 | entropy=-4.21894 | reward=0.25312\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_65.ckpt\n",
      "[INFO]  epoch/step=66/17200 | loss=1.48814 | ploss=0.75554 | vloss=0.73548 | entropy=-4.21118 | reward=0.24906\n",
      "[INFO]  epoch/step=66/17300 | loss=1.37675 | ploss=0.68859 | vloss=0.69106 | entropy=-4.22829 | reward=0.23500\n",
      "[INFO]  epoch/step=66/17400 | loss=1.52992 | ploss=0.76449 | vloss=0.76826 | entropy=-4.15936 | reward=0.26125\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_66.ckpt\n",
      "[INFO]  epoch/step=67/17500 | loss=1.51202 | ploss=0.77508 | vloss=0.73977 | entropy=-4.15411 | reward=0.25000\n",
      "[INFO]  epoch/step=67/17600 | loss=1.43251 | ploss=0.71948 | vloss=0.71588 | entropy=-4.17430 | reward=0.24344\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_67.ckpt\n",
      "[INFO]  epoch/step=68/17700 | loss=1.50557 | ploss=0.75761 | vloss=0.75080 | entropy=-4.16772 | reward=0.25375\n",
      "[INFO]  epoch/step=68/17800 | loss=1.54792 | ploss=0.77328 | vloss=0.77745 | entropy=-4.13868 | reward=0.26438\n",
      "[INFO]  epoch/step=68/17900 | loss=1.48664 | ploss=0.74785 | vloss=0.74161 | entropy=-4.14529 | reward=0.25219\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_68.ckpt\n",
      "[INFO]  epoch/step=69/18000 | loss=1.43468 | ploss=0.72286 | vloss=0.71465 | entropy=-4.15689 | reward=0.24094\n",
      "[INFO]  epoch/step=69/18100 | loss=1.54799 | ploss=0.77244 | vloss=0.77837 | entropy=-4.13654 | reward=0.26469\n",
      "[INFO]  epoch/step=69/18200 | loss=1.54965 | ploss=0.77595 | vloss=0.77653 | entropy=-4.14740 | reward=0.26250\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_69.ckpt\n",
      "[INFO]  epoch/step=70/18300 | loss=1.61688 | ploss=0.81557 | vloss=0.80410 | entropy=-4.11410 | reward=0.27344\n",
      "[INFO]  epoch/step=70/18400 | loss=1.48513 | ploss=0.72883 | vloss=0.75907 | entropy=-4.08718 | reward=0.25812\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_70.ckpt\n",
      "[INFO]  epoch/step=71/18500 | loss=1.37414 | ploss=0.67765 | vloss=0.69933 | entropy=-4.17029 | reward=0.23625\n",
      "[INFO]  epoch/step=71/18600 | loss=1.54137 | ploss=0.76210 | vloss=0.78204 | entropy=-4.09276 | reward=0.26594\n",
      "[INFO]  epoch/step=71/18700 | loss=1.48721 | ploss=0.74476 | vloss=0.74528 | entropy=-4.15024 | reward=0.25344\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_71.ckpt\n",
      "[INFO]  epoch/step=72/18800 | loss=1.49846 | ploss=0.74402 | vloss=0.75723 | entropy=-4.11278 | reward=0.25594\n",
      "[INFO]  epoch/step=72/18900 | loss=1.50889 | ploss=0.74710 | vloss=0.76458 | entropy=-4.11106 | reward=0.26000\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_72.ckpt\n",
      "[INFO]  epoch/step=73/19000 | loss=1.48357 | ploss=0.72854 | vloss=0.75784 | entropy=-4.12635 | reward=0.25562\n",
      "[INFO]  epoch/step=73/19100 | loss=1.54653 | ploss=0.76999 | vloss=0.77928 | entropy=-4.06615 | reward=0.26500\n",
      "[INFO]  epoch/step=73/19200 | loss=1.46947 | ploss=0.72145 | vloss=0.75080 | entropy=-4.08869 | reward=0.25531\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_73.ckpt\n",
      "[INFO]  epoch/step=74/19300 | loss=1.50395 | ploss=0.74826 | vloss=0.75845 | entropy=-4.07949 | reward=0.25688\n",
      "[INFO]  epoch/step=74/19400 | loss=1.48999 | ploss=0.73371 | vloss=0.75907 | entropy=-4.10810 | reward=0.25812\n",
      "[INFO]  epoch/step=74/19500 | loss=1.49105 | ploss=0.73909 | vloss=0.75478 | entropy=-4.12864 | reward=0.25562\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_74.ckpt\n",
      "[INFO]  epoch/step=75/19600 | loss=1.47721 | ploss=0.72366 | vloss=0.75631 | entropy=-4.07793 | reward=0.25719\n",
      "[INFO]  epoch/step=75/19700 | loss=1.45656 | ploss=0.71217 | vloss=0.74712 | entropy=-4.04695 | reward=0.25406\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_75.ckpt\n",
      "[INFO]  epoch/step=76/19800 | loss=1.54075 | ploss=0.75807 | vloss=0.78541 | entropy=-4.04946 | reward=0.26656\n",
      "[INFO]  epoch/step=76/19900 | loss=1.55116 | ploss=0.77091 | vloss=0.78296 | entropy=-4.02040 | reward=0.26625\n",
      "[INFO]  epoch/step=76/20000 | loss=1.49026 | ploss=0.73209 | vloss=0.76091 | entropy=-4.04700 | reward=0.25875\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_76.ckpt\n",
      "[INFO]  epoch/step=77/20100 | loss=1.45726 | ploss=0.71160 | vloss=0.74835 | entropy=-3.99515 | reward=0.25344\n",
      "[INFO]  epoch/step=77/20200 | loss=1.48009 | ploss=0.72376 | vloss=0.75907 | entropy=-4.05411 | reward=0.25812\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_77.ckpt\n",
      "[INFO]  epoch/step=78/20300 | loss=1.53420 | ploss=0.75146 | vloss=0.78541 | entropy=-3.98276 | reward=0.26500\n",
      "[INFO]  epoch/step=78/20400 | loss=1.50423 | ploss=0.72581 | vloss=0.78112 | entropy=-4.01075 | reward=0.26562\n",
      "[INFO]  epoch/step=78/20500 | loss=1.53752 | ploss=0.75910 | vloss=0.78112 | entropy=-4.00802 | reward=0.26562\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_78.ckpt\n",
      "[INFO]  epoch/step=79/20600 | loss=1.59759 | ploss=0.78820 | vloss=0.81206 | entropy=-3.97822 | reward=0.27406\n",
      "[INFO]  epoch/step=79/20700 | loss=1.61989 | ploss=0.79643 | vloss=0.82615 | entropy=-3.99540 | reward=0.28094\n",
      "[INFO]  epoch/step=79/20800 | loss=1.53516 | ploss=0.75427 | vloss=0.78357 | entropy=-3.99286 | reward=0.26438\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_79.ckpt\n",
      "[INFO]  epoch/step=80/20900 | loss=1.52962 | ploss=0.73924 | vloss=0.79307 | entropy=-3.99661 | reward=0.26969\n",
      "[INFO]  epoch/step=80/21000 | loss=1.46269 | ploss=0.72103 | vloss=0.74436 | entropy=-4.00811 | reward=0.25312\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_80.ckpt\n",
      "[INFO]  epoch/step=81/21100 | loss=1.61105 | ploss=0.78572 | vloss=0.82799 | entropy=-3.97415 | reward=0.28000\n",
      "[INFO]  epoch/step=81/21200 | loss=1.51634 | ploss=0.72956 | vloss=0.78939 | entropy=-3.91730 | reward=0.26844\n",
      "[INFO]  epoch/step=81/21300 | loss=1.41609 | ploss=0.68722 | vloss=0.73150 | entropy=-3.93820 | reward=0.24875\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_81.ckpt\n",
      "[INFO]  epoch/step=82/21400 | loss=1.38305 | ploss=0.66251 | vloss=0.72323 | entropy=-3.98655 | reward=0.24438\n",
      "[INFO]  epoch/step=82/21500 | loss=1.50059 | ploss=0.73313 | vloss=0.77009 | entropy=-3.94579 | reward=0.26188\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_82.ckpt\n",
      "[INFO]  epoch/step=83/21600 | loss=1.51932 | ploss=0.73564 | vloss=0.78633 | entropy=-3.95968 | reward=0.26531\n",
      "[INFO]  epoch/step=83/21700 | loss=1.46909 | ploss=0.70349 | vloss=0.76826 | entropy=-3.95496 | reward=0.26125\n",
      "[INFO]  epoch/step=83/21800 | loss=1.54667 | ploss=0.74519 | vloss=0.80410 | entropy=-3.91331 | reward=0.27344\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_83.ckpt\n",
      "[INFO]  epoch/step=84/21900 | loss=1.49450 | ploss=0.70802 | vloss=0.78909 | entropy=-3.90007 | reward=0.26781\n",
      "[INFO]  epoch/step=84/22000 | loss=1.45502 | ploss=0.69763 | vloss=0.75999 | entropy=-3.90451 | reward=0.25844\n",
      "[INFO]  epoch/step=84/22100 | loss=1.52696 | ploss=0.72638 | vloss=0.80318 | entropy=-3.89759 | reward=0.27156\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_84.ckpt\n",
      "[INFO]  epoch/step=85/22200 | loss=1.43319 | ploss=0.67855 | vloss=0.75723 | entropy=-3.88969 | reward=0.25750\n",
      "[INFO]  epoch/step=85/22300 | loss=1.39527 | ploss=0.65257 | vloss=0.74528 | entropy=-3.88597 | reward=0.25344\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_85.ckpt\n",
      "[INFO]  epoch/step=86/22400 | loss=1.47093 | ploss=0.70280 | vloss=0.77071 | entropy=-3.88142 | reward=0.26156\n",
      "[INFO]  epoch/step=86/22500 | loss=1.45574 | ploss=0.69004 | vloss=0.76826 | entropy=-3.86170 | reward=0.26125\n",
      "[INFO]  epoch/step=86/22600 | loss=1.50738 | ploss=0.72603 | vloss=0.78388 | entropy=-3.82664 | reward=0.26656\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_86.ckpt\n",
      "[INFO]  epoch/step=87/22700 | loss=1.52890 | ploss=0.73657 | vloss=0.79491 | entropy=-3.87399 | reward=0.26875\n",
      "[INFO]  epoch/step=87/22800 | loss=1.57350 | ploss=0.74804 | vloss=0.82799 | entropy=-3.82796 | reward=0.28156\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_87.ckpt\n",
      "[INFO]  epoch/step=88/22900 | loss=1.46642 | ploss=0.70871 | vloss=0.76029 | entropy=-3.87146 | reward=0.25750\n",
      "[INFO]  epoch/step=88/23000 | loss=1.50943 | ploss=0.72167 | vloss=0.79031 | entropy=-3.84841 | reward=0.26875\n",
      "[INFO]  epoch/step=88/23100 | loss=1.54607 | ploss=0.73163 | vloss=0.81696 | entropy=-3.81726 | reward=0.27781\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_88.ckpt\n",
      "[INFO]  epoch/step=89/23200 | loss=1.52868 | ploss=0.71730 | vloss=0.81390 | entropy=-3.81404 | reward=0.27469\n",
      "[INFO]  epoch/step=89/23300 | loss=1.46168 | ploss=0.68770 | vloss=0.77653 | entropy=-3.83766 | reward=0.26406\n",
      "[INFO]  epoch/step=89/23400 | loss=1.49783 | ploss=0.70358 | vloss=0.79674 | entropy=-3.79206 | reward=0.26937\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_89.ckpt\n",
      "[INFO]  epoch/step=90/23500 | loss=1.51920 | ploss=0.72596 | vloss=0.79583 | entropy=-3.87917 | reward=0.27063\n",
      "[INFO]  epoch/step=90/23600 | loss=1.49949 | ploss=0.69789 | vloss=0.80410 | entropy=-3.78626 | reward=0.27344\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_90.ckpt\n",
      "[INFO]  epoch/step=91/23700 | loss=1.43349 | ploss=0.68395 | vloss=0.75202 | entropy=-3.76758 | reward=0.25312\n",
      "[INFO]  epoch/step=91/23800 | loss=1.54361 | ploss=0.71903 | vloss=0.82707 | entropy=-3.78856 | reward=0.28125\n",
      "[INFO]  epoch/step=91/23900 | loss=1.47308 | ploss=0.69445 | vloss=0.78112 | entropy=-3.78750 | reward=0.26562\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_91.ckpt\n",
      "[INFO]  epoch/step=92/24000 | loss=1.53090 | ploss=0.71548 | vloss=0.81788 | entropy=-3.75350 | reward=0.27656\n",
      "[INFO]  epoch/step=92/24100 | loss=1.44741 | ploss=0.66878 | vloss=0.78112 | entropy=-3.78013 | reward=0.26562\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_92.ckpt\n",
      "[INFO]  epoch/step=93/24200 | loss=1.55205 | ploss=0.72498 | vloss=0.82952 | entropy=-3.74231 | reward=0.28000\n",
      "[INFO]  epoch/step=93/24300 | loss=1.46700 | ploss=0.69023 | vloss=0.77928 | entropy=-3.80130 | reward=0.26500\n",
      "[INFO]  epoch/step=93/24400 | loss=1.51950 | ploss=0.69762 | vloss=0.82431 | entropy=-3.72980 | reward=0.28031\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_93.ckpt\n",
      "[INFO]  epoch/step=94/24500 | loss=1.54327 | ploss=0.73303 | vloss=0.81267 | entropy=-3.72158 | reward=0.27531\n",
      "[INFO]  epoch/step=94/24600 | loss=1.52819 | ploss=0.72473 | vloss=0.80593 | entropy=-3.76546 | reward=0.27406\n",
      "[INFO]  epoch/step=94/24700 | loss=1.47005 | ploss=0.68984 | vloss=0.78265 | entropy=-3.73141 | reward=0.26406\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_94.ckpt\n",
      "[INFO]  epoch/step=95/24800 | loss=1.49917 | ploss=0.70120 | vloss=0.80042 | entropy=-3.73529 | reward=0.27219\n",
      "[INFO]  epoch/step=95/24900 | loss=1.46066 | ploss=0.68385 | vloss=0.77928 | entropy=-3.76446 | reward=0.26500\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_95.ckpt\n",
      "[INFO]  epoch/step=96/25000 | loss=1.57276 | ploss=0.72419 | vloss=0.85096 | entropy=-3.68923 | reward=0.28781\n",
      "[INFO]  epoch/step=96/25100 | loss=1.50130 | ploss=0.70792 | vloss=0.79583 | entropy=-3.73622 | reward=0.27063\n",
      "[INFO]  epoch/step=96/25200 | loss=1.52782 | ploss=0.72065 | vloss=0.80961 | entropy=-3.72838 | reward=0.27531\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_96.ckpt\n",
      "[INFO]  epoch/step=97/25300 | loss=1.62969 | ploss=0.74989 | vloss=0.88221 | entropy=-3.69583 | reward=0.29688\n",
      "[INFO]  epoch/step=97/25400 | loss=1.47136 | ploss=0.67985 | vloss=0.79399 | entropy=-3.76228 | reward=0.27000\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_97.ckpt\n",
      "[INFO]  epoch/step=98/25500 | loss=1.46576 | ploss=0.67451 | vloss=0.79368 | entropy=-3.72252 | reward=0.26781\n",
      "[INFO]  epoch/step=98/25600 | loss=1.55586 | ploss=0.72295 | vloss=0.83534 | entropy=-3.72064 | reward=0.28406\n",
      "[INFO]  epoch/step=98/25700 | loss=1.44057 | ploss=0.65457 | vloss=0.78847 | entropy=-3.75554 | reward=0.26813\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_98.ckpt\n",
      "[INFO]  epoch/step=99/25800 | loss=1.46672 | ploss=0.68283 | vloss=0.78633 | entropy=-3.72313 | reward=0.26531\n",
      "[INFO]  epoch/step=99/25900 | loss=1.43604 | ploss=0.66194 | vloss=0.77653 | entropy=-3.71497 | reward=0.26406\n",
      "[INFO]  epoch/step=99/26000 | loss=1.52300 | ploss=0.71640 | vloss=0.80900 | entropy=-3.67721 | reward=0.27406\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_99.ckpt\n",
      "[INFO]  epoch/step=100/26100 | loss=1.49166 | ploss=0.70102 | vloss=0.79307 | entropy=-3.71464 | reward=0.26969\n",
      "[INFO]  epoch/step=100/26200 | loss=1.49220 | ploss=0.69146 | vloss=0.80318 | entropy=-3.72161 | reward=0.27313\n",
      "[INFO]  Save model to ../save_model/Amazon_Beauty_Core/lstm/note_rm_w1_p0_g_aiu_0_0_300/policy_model_epoch_100.ckpt\n",
      "[INFO]  current time = 20210526-171923\n"
     ]
    }
   ],
   "source": [
    "command = f\"python3 ../src/{train_file} --reasoning_step {reasoning_step} \\\n",
    "                                         --batch_size {batch_size} \\\n",
    "                                         --name {exp_name} \\\n",
    "                                         --lr {lr} \\\n",
    "                                         --embed_size {embed_size} \\\n",
    "                                         --n_memory {n_memory} \\\n",
    "                                         --load_pretrain_model {load_pretrain_model}  \\\n",
    "                                         --tri_wd_rm {tri_wd_rm} \\\n",
    "                                         --tri_pro_rm {tri_pro_rm} \\\n",
    "                                         --gp_setting {gp_setting} \\\n",
    "                                         --epochs {epochs} \\\n",
    "                                         --KGE_pretrained {KGE_pretrained} \\\n",
    "                                         --lambda_num {lambda_num}  \\\n",
    "                                         --kg_emb_grad {kg_emb_grad} \\\n",
    "                                         --p_hop {p_hop}  \\\n",
    "                                         --reasoning_step {reasoning_step}  \\\n",
    "                                         --model lstm \\\n",
    "                                         --dataset {DATASET}\"\n",
    "print(\"Running Command:\")\n",
    "print(' '.join(command.split()))\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-pearl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 ../src/test.py --name note_rm_w1_p0 --batch_size 32 --gp_setting 6_800_15_500_50 --model lstm --dataset beauty_core --lambda_num 0.5 --kg_emb_grad 0 --lr 0.0001 --tri_wd_rm 1 --tri_pro_rm 0 --p_hop 2 --reasoning_step 2 --embed_size 32 --save_pretrain_model 1 --n_memory 64\n",
      "args.gp_setting =  6_800_15_500_50 args.att_core =  0 args.item_core =  0 args.user_core_th =  6 args.kg_fre_upper =  500 args.max_acts =  50\n",
      "label_file =  ../data/Amazon_Beauty_Core/train_label.pkl\n",
      "label_file =  ../data/Amazon_Beauty_Core/test_label.pkl\n",
      "start predict\n",
      "Predicting paths...\n",
      "len(self.core_user_list) =  8300\n",
      "self.args.core_user_list =  8300\n",
      "self.user_triplet_list =  8300\n",
      "Load embedding: ../data/Amazon_Beauty_Core/transe_embed.pkl\n",
      "r =  setup self_loop\n",
      "r =  setup purchase\n",
      "r =  setup mentions\n",
      "r =  setup described_as\n",
      "r =  setup produced_by\n",
      "r =  setup belongs_to\n",
      "r =  setup also_bought\n",
      "r =  setup also_viewed\n",
      "r =  setup bought_together\n",
      "r =  setup padding\n",
      "  9%|███▌                                  | 784/8300 [23:51<3:48:03,  1.82s/it][INFO]  'batch_uids = ', 10815,11206,9502,2818,14165,1914,12441,12009,21718,7678,16967,177,15301,10351,19095,2930, 800, 800\n",
      " 11%|████                                  | 880/8300 [26:49<3:47:34,  1.84s/it]"
     ]
    }
   ],
   "source": [
    "save_pretrain_model = 1\n",
    "\n",
    "command = f\"python3 ../src/{test_file} --name {exp_name} \\\n",
    "                                        --batch_size {batch_size} \\\n",
    "                                        --gp_setting {gp_setting} \\\n",
    "                                        --model lstm \\\n",
    "                                        --dataset {DATASET} \\\n",
    "                                        --lambda_num {lambda_num} \\\n",
    "                                        --kg_emb_grad {kg_emb_grad} \\\n",
    "                                        --lr {lr} \\\n",
    "                                         --tri_wd_rm {tri_wd_rm} \\\n",
    "                                         --tri_pro_rm {tri_pro_rm} \\\n",
    "                                        --p_hop {p_hop} \\\n",
    "                                        --reasoning_step {reasoning_step} \\\n",
    "                                        --embed_size {embed_size} \\\n",
    "                                        --save_pretrain_model {save_pretrain_model}\\\n",
    "                                        --n_memory {n_memory}\"\n",
    "\n",
    "print(' '.join(command.split()))\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-behavior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "underlying-harris",
   "metadata": {},
   "source": [
    "# train UCPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrain_model=1\n",
    "\n",
    "tri_wd_rm=0\n",
    "tri_pro_rm=0\n",
    "exp_name=f\"note_rm_w{tri_wd_rm}_p{tri_pro_rm}\"\n",
    "\n",
    "command = f\"python3 ../src/{train_file} --reasoning_step {reasoning_step} \\\n",
    "                                         --batch_size {batch_size} \\\n",
    "                                         --name {exp_name} \\\n",
    "                                         --lr {lr} \\\n",
    "                                         --embed_size {embed_size} \\\n",
    "                                         --n_memory {n_memory} \\\n",
    "                                         --load_pretrain_model {load_pretrain_model}  \\\n",
    "                                         --KGE_pretrained {KGE_pretrained} \\\n",
    "                                         --tri_wd_rm {tri_wd_rm} \\\n",
    "                                         --tri_pro_rm {tri_pro_rm} \\\n",
    "                                         --gp_setting {gp_setting} \\\n",
    "                                         --epochs {epochs} \\\n",
    "                                         --KGE_pretrained {KGE_pretrained} \\\n",
    "                                         --lambda_num {lambda_num}  \\\n",
    "                                         --kg_emb_grad {kg_emb_grad} \\\n",
    "                                         --p_hop {p_hop}  \\\n",
    "                                         --reasoning_step {reasoning_step}  \\\n",
    "                                         --model {model} \\\n",
    "                                         --dataset {DATASET}\"\n",
    "print(\"Running Command:\")\n",
    "print(' '.join(command.split()))\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"python3 ../src/{test_file} --name {exp_name} \\\n",
    "                                        --batch_size {batch_size} \\\n",
    "                                        --gp_setting {gp_setting} \\\n",
    "                                        --model {model} \\\n",
    "                                        --dataset {DATASET} \\\n",
    "                                        --lambda_num {lambda_num} \\\n",
    "                                        --kg_emb_grad {kg_emb_grad} \\\n",
    "                                        --lr {lr} \\\n",
    "                                         --tri_wd_rm {tri_wd_rm} \\\n",
    "                                         --tri_pro_rm {tri_pro_rm} \\\n",
    "                                        --p_hop {p_hop} \\\n",
    "                                        --reasoning_step {reasoning_step} \\\n",
    "                                        --embed_size {embed_size} \\\n",
    "                                        --n_memory {n_memory}\"\n",
    "\n",
    "print(' '.join(command.split()))\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-specialist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-project",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-barrier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-winning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
